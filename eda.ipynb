{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table = pd.read_csv(\"MainTable.csv\")\n",
    "subjects_table = pd.read_csv(\"Subject.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_table.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "problem_counts = main_table.groupby(['SubjectID', 'ProblemID']).size().reset_index(name='problem_count')\n",
    "\n",
    "average_problems = problem_counts.groupby('SubjectID')['problem_count'].mean().reset_index(name='average_problems')\n",
    "result_df = pd.merge(subjects_table, average_problems, on='SubjectID', how='left')\n",
    "result_df = result_df[result_df[\"X-Grade\"] != 0]\n",
    "result_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result_df[\"average_problems\"], result_df[\"X-Grade\"])\n",
    "\n",
    "plt.xlabel(\"Average # of Problem Attempts\")\n",
    "plt.ylabel(\"Grade\")\n",
    "plt.title(\"Scatter Plot of Grade vs Average # of Problem Attempts\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count occurrences of \"Error\" in Compile.Result for each SubjectID\n",
    "error_counts = main_table[main_table[\"EventType\"] == \"Compile.Error\"].groupby(\"SubjectID\").size().reset_index(name=\"error_count\")\n",
    "\n",
    "# Compute the average number of errors per SubjectID\n",
    "average_errors = error_counts.groupby(\"SubjectID\")[\"error_count\"].mean().reset_index(name=\"average_errors\")\n",
    "\n",
    "# Merge with subjects_table to retain SubjectID and grades\n",
    "result_df = pd.merge(subjects_table, average_errors, on=\"SubjectID\", how=\"left\")\n",
    "\n",
    "# Drop rows where 'X-Grade' is 0\n",
    "result_df = result_df[result_df[\"X-Grade\"] != 0]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(result_df[\"average_errors\"], result_df[\"X-Grade\"])\n",
    "\n",
    "plt.xlabel(\"Average # of Compilation Errors\")\n",
    "plt.ylabel(\"Grade\")\n",
    "plt.title(\"Scatter Plot of Grade vs Average # of Compilation Errors\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count the number of attempts per problem\n",
    "attempts_per_problem = main_table.groupby(\"ProblemID\").size().reset_index(name=\"total_attempts\")\n",
    "\n",
    "# Step 2: Count the number of unique students who attempted each problem\n",
    "unique_students_per_problem = main_table.groupby(\"ProblemID\")[\"SubjectID\"].nunique().reset_index(name=\"num_students\")\n",
    "\n",
    "# Step 3: Calculate the average number of attempts per student for each problem\n",
    "average_attempts_per_problem = pd.merge(attempts_per_problem, unique_students_per_problem, on=\"ProblemID\")\n",
    "average_attempts_per_problem[\"avg_attempts_per_question\"] = average_attempts_per_problem[\"total_attempts\"] / average_attempts_per_problem[\"num_students\"]\n",
    "\n",
    "# Step 4: Plot the average attempts per problem\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(average_attempts_per_problem[\"ProblemID\"].astype(str), average_attempts_per_problem[\"avg_attempts_per_question\"], color='lightcoral', alpha=0.7)\n",
    "\n",
    "# Step 5: Add labels and title\n",
    "plt.xlabel(\"Problem ID\")\n",
    "plt.ylabel(\"Average Number of Attempts\")\n",
    "plt.title(\"Average Number of Attempts per Question\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "\n",
    "# Step 6: Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Group the data by SubjectID and ProblemID to count the number of attempts\n",
    "attempts_per_student_problem = main_table.groupby(['SubjectID', 'ProblemID']).size().reset_index(name='attempts')\n",
    "\n",
    "# Step 2: Pivot the data to have SubjectID as rows and ProblemID as columns\n",
    "attempts_per_student_problem_pivot = attempts_per_student_problem.pivot(index='SubjectID', columns='ProblemID', values='attempts').fillna(0)\n",
    "\n",
    "# Step 3: Display the DataFrame with the number of attempts per student per problem\n",
    "attempts_per_student_problem_pivot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Merge the attempts dataset with subjects_table on SubjectID\n",
    "merged_df = pd.merge(subjects_table, attempts_per_student_problem_pivot, on=\"SubjectID\", how=\"left\")\n",
    "merged_df= merged_df[merged_df[\"X-Grade\"] != 0]\n",
    "\n",
    "# Step 2: Normalize the data (only the problem attempts)\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(merged_df.drop(columns=[\"SubjectID\", \"X-Grade\"]))  # Drop non-relevant columns\n",
    "\n",
    "# Step 3: Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "merged_df['Cluster'] = kmeans.fit_predict(normalized_data)\n",
    "\n",
    "# Step 4: Calculate the average X-Grade for each cluster\n",
    "cluster_avg_grades = merged_df.groupby('Cluster')['X-Grade'].mean().reset_index()\n",
    "\n",
    "# Step 5: Output the average X-Grade for each cluster\n",
    "print(cluster_avg_grades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the standard deviation of X-Grade for each cluster\n",
    "cluster_std_dev = merged_df.groupby('Cluster')['X-Grade'].std().reset_index()\n",
    "\n",
    "# Step 2: Output the standard deviation for each cluster\n",
    "print(cluster_std_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sum the number of attempts for each student across problems\n",
    "# Assuming that 'attempts_per_student_problem_pivot' contains the number of attempts for each student and problem\n",
    "attempts_sum = merged_df.drop(columns=[\"SubjectID\", \"X-Grade\", \"Cluster\"]).sum(axis=1)\n",
    "\n",
    "# Step 2: Count the number of problems (columns) for each student\n",
    "# Here we count the number of non-NaN values (problem attempts) for each student\n",
    "num_problems = merged_df.drop(columns=[\"SubjectID\", \"X-Grade\", \"Cluster\"]).notna().sum(axis=1)\n",
    "\n",
    "# Step 3: Calculate the average number of attempts for each student\n",
    "merged_df['average_problems'] = attempts_sum / num_problems\n",
    "\n",
    "# Step 4: Plot the Grades vs Average Number of Attempts with Cluster-based coloring\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(\n",
    "    merged_df['average_problems'],  # x-axis: Average Number of Attempts\n",
    "    merged_df['X-Grade'],  # y-axis: X-Grade (Grades)\n",
    "    c=merged_df['Cluster'],  # Color by Cluster\n",
    "    cmap='viridis',  # You can change the colormap (e.g., 'viridis', 'plasma', 'coolwarm')\n",
    "    alpha=0.7  # Transparency level for the dots\n",
    ")\n",
    "\n",
    "# Add colorbar to show which color corresponds to which cluster\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Step 5: Add labels and title to the plot\n",
    "plt.xlabel(\"Average # of Problem Attempts\")\n",
    "plt.ylabel(\"Grade (X-Grade)\")\n",
    "plt.title(\"Scatter Plot: Grade vs Average # of Problem Attempts (Colored by Clusters)\")\n",
    "\n",
    "# Step 6: Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Merge the attempts dataset with subjects_table on SubjectID\n",
    "merged_df = pd.merge(subjects_table, attempts_per_student_problem_pivot, on=\"SubjectID\", how=\"left\")\n",
    "merged_df = merged_df[merged_df[\"X-Grade\"] != 0]\n",
    "\n",
    "# Step 2: Normalize the data (only the problem attempts)\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(merged_df.drop(columns=[\"SubjectID\", \"X-Grade\"]))  # Drop non-relevant columns\n",
    "\n",
    "# Step 3: Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "merged_df['Cluster'] = kmeans.fit_predict(normalized_data)\n",
    "\n",
    "# Step 4: Apply PCA on the normalized data to reduce the problem attempts to 1 principal component\n",
    "pca = PCA(n_components=1)\n",
    "pc1 = pca.fit_transform(normalized_data)\n",
    "\n",
    "# Step 5: Add the PCA result (PC1) back to the merged dataframe\n",
    "merged_df['PC1'] = pc1\n",
    "\n",
    "# Step 6: Plot the first principal component (PC1) against X-Grade, colored by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(merged_df['PC1'], merged_df['X-Grade'], c=merged_df['Cluster'], cmap='viridis', alpha=0.7)\n",
    "\n",
    "# Step 7: Add color bar to show the cluster numbers\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# Step 8: Add labels and title to the plot\n",
    "plt.xlabel(\"Principal Component 1 (PC1) of Problem Attempts\")\n",
    "plt.ylabel(\"Grade (X-Grade)\")\n",
    "plt.title(\"Scatter Plot: Grade vs PCA of Problem Attempts (PC1), Colored by Clusters\")\n",
    "\n",
    "# Step 9: Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optionally, print the explained variance ratio to understand how much variance is explained by PC1\n",
    "print(f\"Explained variance ratio for PC1: {pca.explained_variance_ratio_[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if some errors are harder to shake /lead to lower grades\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load datasets\n",
    "main_df = pd.read_csv(\"MainTable.csv\")\n",
    "subject_df = pd.read_csv(\"Subject.csv\")\n",
    "\n",
    "# Step 1: Extract only the error message (remove line numbers)\n",
    "def clean_compile_message(message):\n",
    "    if pd.isna(message):  # Handle NaN values\n",
    "        return None\n",
    "    return re.sub(r'line \\d+: ', '', message).strip()\n",
    "\n",
    "def classify_error(error_message):\n",
    "    if error_message is None:  # Handle None values\n",
    "        return \"Other\"\n",
    "\n",
    "    # Define keyword-based matching for classification\n",
    "    error_categories = {\n",
    "        \"Syntax Error\": [r\"expected\", r\"missing\", r\"'.class' expected\"],\n",
    "        \"Reference Error\": [r\"cannot find symbol\", r\"no suitable method\", r\"method .* cannot be applied\"],\n",
    "        \"Type Error\": [r\"incompatible types\", r\"bad operand\", r\"required, but\"],\n",
    "        \"Initialization Error\": [r\"might not have been initialized\", r\"already defined\"],\n",
    "        \"Illegal Character\": [r\"illegal character\", r\"illegal '.'\"],\n",
    "        \"Numeric Error\": [r\"integer number too large\", r\"array dimension missing\"]\n",
    "    }\n",
    "    \n",
    "    for category, patterns in error_categories.items():\n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, error_message, re.IGNORECASE):\n",
    "                return category\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "\n",
    "main_df['CompileMessageData'] = main_df['CompileMessageData'].apply(clean_compile_message)\n",
    "main_df['CompileMessageData'] = main_df['CompileMessageData'].apply(classify_error)\n",
    "\n",
    "# Step 2: Merge datasets on 'SubjectID'\n",
    "merged_df = main_df.merge(subject_df, on=\"SubjectID\", how=\"left\")\n",
    "\n",
    "# Step 3: Count occurrences of each error type per SubjectID\n",
    "error_counts = merged_df.groupby(['SubjectID', 'CompileMessageData']).size().unstack(fill_value=0)\n",
    "\n",
    "# Step 4: Merge error counts with X-Grade\n",
    "final_df = error_counts.merge(subject_df, on=\"SubjectID\", how=\"left\")\n",
    "\n",
    "\n",
    "# Step 5: Convert X-Grade to numeric\n",
    "final_df['X-Grade'] = pd.to_numeric(final_df['X-Grade'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------- üî• Visualization üî• --------------------\n",
    "\n",
    "# 1Ô∏è‚É£ Compute average grade for students who made each error vs. those who didn't\n",
    "error_effects = {}\n",
    "for error in error_counts.columns:\n",
    "    students_with_error = final_df[final_df[error] > 0]\n",
    "    students_without_error = final_df[final_df[error] == 0]\n",
    "\n",
    "    avg_with_error = students_with_error[\"X-Grade\"].mean()\n",
    "    avg_without_error = students_without_error[\"X-Grade\"].mean()\n",
    "\n",
    "    error_effects[error] = avg_with_error - avg_without_error  # Difference in grades\n",
    "\n",
    "# Convert to DataFrame\n",
    "error_effects_df = pd.DataFrame(error_effects.items(), columns=[\"Error Type\", \"Grade Impact\"])\n",
    "error_effects_df = error_effects_df.sort_values(\"Grade Impact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a threshold to filter out rare errors\n",
    "min_occurrences = 10  # Adjust this based on dataset size\n",
    "\n",
    "# Count how many students made each error\n",
    "error_frequencies = error_counts.sum(axis=0)\n",
    "\n",
    "# Merge with grade impact data and filter by occurrence count\n",
    "filtered_errors = error_effects_df.merge(error_frequencies.rename(\"Frequency\"), left_on=\"Error Type\", right_index=True)\n",
    "filtered_errors = filtered_errors[filtered_errors[\"Frequency\"] >= min_occurrences]\n",
    "\n",
    "# Sort by most negative grade impact and keep top 10\n",
    "filtered_errors = filtered_errors.sort_values(\"Grade Impact\").head(10)\n",
    "\n",
    "# üî• Optimized Bar Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=filtered_errors[\"Error Type\"], y=filtered_errors[\"Grade Impact\"], palette=\"coolwarm\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Grade Impact (Avg Grade Difference)\")\n",
    "plt.title(\"Most Harmful Errors on Student Grades\")\n",
    "plt.axhline(0, color='black', linewidth=1)  # Reference line at zero\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all unique error types\n",
    "unique_errors = error_counts.columns.tolist()\n",
    "print(\"Unique Error Types:\")\n",
    "for error in unique_errors:\n",
    "    print(error)\n",
    "\n",
    "# Check how many different errors there are\n",
    "print(f\"\\nTotal unique error types: {len(unique_errors)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3Ô∏è‚É£ Scatter plot: Does making an error more often lower grades?\n",
    "top_errors_list = error_effects_df[\"Error Type\"].head(4)  # Pick top 4 most harmful errors\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, error in enumerate(top_errors_list):\n",
    "    sns.regplot(x=final_df[error], y=final_df[\"X-Grade\"], ax=axes[i], scatter_kws={'alpha':0.5})\n",
    "    axes[i].set_title(f\"X-Grade vs. '{error}' Occurrences\")\n",
    "    axes[i].set_xlabel(f\"Count of '{error}'\")\n",
    "    axes[i].set_ylabel(\"X-Grade\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
